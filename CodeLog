Test 7 isn't consistent.
=-=2 - Fri Dec  4 18:17:19 PST 2009

Toy examples done. url -> related docs.
Data structures now starting to take shape.

Tests are ugly, code interleaved with data.
We can use more indexes.
Decouple the index like in SQL, so the same code runs faster if the index
exists.
=-=4 - Fri Dec  4 19:14:31 PST 2009

Finished building the index. gen-docs works.
=-=5 - Sat Dec  5 09:47:15 PST 2009
Added agg3 ui to it. Messy that I've added stations to the flat-history model.
  Form to create new station.
  Choose between stations, each station gets its own read-list.
=-=6 - Mon Dec  7 13:22:11 PST 2009

Importing agg3 crawl pipeline. Next step: merge properly.

Vision: crawl a universal set of feeds. Don't wait for entire crawl to import
crawled feeds. Track per-user feeds and feed priorities in userinfo*. Make
agg3 just another station for users who import feeds. Push station selection
to a new screen; add new station form to the left panel when reading. Track
history by station.

Later: screens to manage feeds. Stations for importing email, other data
sources, etc. Discovering new feeds during crawl.
=-=7 - Tue Dec  8 12:06:08 PST 2009

crawl and htmlclean now communicate by fifo.
But it's currently blocking. So htmlclean blocks until input, then just reads
the first input and goes back to waiting. Meanwhile if I add a delay to
htmlclean it blocks crawl instead.
=-=8 - Tue Dec  8 14:11:15 PST 2009
If we stick with the blocking model we need to start the pipeline in reverse
order, from consumer to producer of each fifo.

If any stage dies, all other stages will block.

Will we lose stuff in fifos if we kill arc? Periodically look at files under
urls/
=-=9 - Tue Dec  8 15:06:09 PST 2009
Redefine defscan to run in separate threads, not a single thread.
Also some reorg.
=-=10 - Tue Dec  8 18:16:09 PST 2009

3 Crawl errors fixed.
=-=11 - Tue Dec  8 19:34:06 PST 2009

Instead of printing progress for background threads, maintain a circular
buffer of messages for each.
Not persistent for now.
=-=12 - Tue Dec  8 22:34:02 PST 2009

Focusing on UI.
Yesterday's problem: we want to scan read-list for both gen-docs and rendering
history.
=-=14 - Tue Dec  8 22:48:22 PST 2009

Ok, partial matches now working. Takes too long, though. Do all the work in
one pass. Every page load should do just one pass through docinfo*.
=-=15 - Tue Dec  8 23:24:59 PST 2009
Single pass for site-docs and feed-docs takes test runtime down from 72s to
42s for 13650 items in docinfo. But it's still too much.
=-=16 - Tue Dec  8 23:54:18 PST 2009
All that time is being spent on downcase!
Just add downcase versions of all metadata.
Or cache downcase separately.
=-=17 - Wed Dec  9 00:00:57 PST 2009
Yep, cached downcase and got to within 3x of non-downcase version.
=-=18 - Wed Dec  9 00:04:45 PST 2009

Time to start indexing. It's working, but dies every now and then. I think
it's dying when the pipe breaks.
=-=19 - Wed Dec  9 00:35:19 PST 2009
Ok, found the core dump in keywords.cc - negative index.
=-=21 - Thu Dec 10 00:44:08 PST 2009
Cleanup.
=-=22 - Thu Dec 10 00:46:41 PST 2009

Back to the UI. Refactoring index.arc to always take a user arg.
gen-docs doesn't like docs as arguments. I need to test-drive this..
=-=23 - Fri Dec 11 04:55:14 PST 2009
Done. Rebuilding the index.
=-=24 - Fri Dec 11 08:10:24 PST 2009

Lots of reorg, getting tests to pass.
Framework for gen-docs:
  generate candidates using doc-filters* and misc-filters*
  constraints to filter candidates
  score remaining candidates
=-=27 - Fri Dec 11 22:41:22 PST 2009

Trying to redo transparent persistence.
  Avoid unnecessary writes based on palsecam's idea.
  Write to new files so we have backups, and to eliminate any chance of data loss.
Doesn't work yet. I just realized save-registry needs to store references to
variables, not names.
Which means if I assign to a persistent variable I will lose persistence on it.
=-=30 - Sun Dec 13 19:53:37 PST 2009
Ok, save-registry now indexes by reference. But still not working. What's
going on?

Two bugs:
  a) (hash key) searches by value, even if what is stored in the hash is a
  reference. What's more it's searching by value at the time of insertion.
  Weird.
  b) buffered-exec isn't wiping when it should.
=-=31 - Sun Dec 13 20:43:03 PST 2009
Ah, it's a *hash* table. It's storing by reference but the hash is computed by
value.
  http://arclanguage.org/item?id=10984
It just shouldn't store by reference. That makes it too confusing.

Got the new transparent persistence working.
=-=32 - Mon Dec 14 01:23:05 PST 2009
Bugfix: snapshots were never being loaded.
=-= - Mon Dec 14 11:30:12 PST 2009

Getting readwarp up on AWS.
After all that testing turns out I never tested the metadata scan. It was
emitting strings with quotes escaped. Fixed.
=-=33 - Wed Dec 16 01:30:36 PST 2009
More bugfixing.
=-=36 - Thu Dec 17 17:18:00 PST 2009
Compute affinity graph by feed rather than url.

But it was taking too long; dhash wasn't working right. Ah, I never did
implement the nilcache idea to avoid recomputation.
Bring back old state.arc tests as well.
=-=37 - Thu Dec 17 19:55:41 PST 2009
Still struggling to compute feed affinity graph.
It's too slow. Let's switch to shell pipes. Dev speed includes performance
when experiments are this intensive and iterations this frequent.
=-=39 - Fri Dec 18 21:21:02 PST 2009

New python script to try normalizing overlap by keyword between feeds - each
keyword distributes 1/n to n*n feed connections.
We're seeing some quality. Krugman's blog is kinda well-connected to financial
blogs or blogs with kinda financial content.

Next steps:
  a signal for article-article overlap to help get at financial posts within
  say larry cheng's feed
  improve html cleaning to remove spurious overlap

Let's see if we can get by without changing keywords algo.
=-=41 - Sat Dec 19 15:29:31 PST 2009

Focusing on html cleaning now. First step: workflow to add tests to a corpus.
=-=42 - Sun Dec 20 15:25:27 PST 2009
Test script up and running.
=-=46 - Sun Dec 20 19:42:33 PST 2009
Added failing test cases by searching for commonly-occurring sets of keywords.
Just make one test case from each such set, that gives us 53 failing tests.
Ok, we've now got our work cut out for us.
=-=49 - Mon Dec 21 01:01:44 PST 2009

I need to add hints from feed descriptions. Step 1: crawl feeds to save
descriptions.
=-=50 - Tue Dec 22 12:23:03 PST 2009
Step 2: use the deschint: if it exists return the highest-scoring node that
contains the hint as a substring.

Unicode errors; I'm going over this tutorial:
  http://docs.python.org/howto/unicode.html
=-=51 - Tue Dec 22 13:55:41 PST 2009
=-=52 - Tue Dec 22 20:16:06 PST 2009
Turns out sorting by score then searching for a superset of deschint isn't
working.
=-=53 - Wed Dec 23 00:12:14 PST 2009
